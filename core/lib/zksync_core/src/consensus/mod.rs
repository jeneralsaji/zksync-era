//! Consensus-related functionality.

#![allow(clippy::redundant_locals)]
#![allow(clippy::needless_pass_by_ref_mut)]
use std::sync::Arc;

use anyhow::Context as _;
use tokio::sync::watch;
use zksync_concurrency::{ctx, error::Wrap as _, limiter, scope, time};
use zksync_consensus_executor as executor;
use zksync_consensus_roles::validator;
use zksync_consensus_storage::BlockStore;
use zksync_dal::{ConnectionPool, Core};

use crate::sync_layer::{sync_action::ActionQueueSender, MainNodeClient, SyncState};

pub use self::{fetcher::*, storage::Store};

mod config;
mod fetcher;
mod storage;
#[cfg(test)]
pub(crate) mod testonly;
#[cfg(test)]
mod tests;

pub use config::{Config, Secrets};

pub async fn run_main_node(
    ctx: &ctx::Ctx,
    cfg: MainNodeConfig,
    pool: ConnectionPool<Core>,
    mut stop_receiver: watch::Receiver<bool>,
) -> anyhow::Result<()> {
    scope::run!(ctx, |ctx, s| async {
        s.spawn_bg(async {
            // Consensus is a new component.
            // For now in case of error we just log it and allow the server
            // to continue running.
            if let Err(err) = cfg.run(ctx, Store(pool)).await {
                tracing::error!(%err, "Consensus actor failed");
            } else {
                tracing::info!("Consensus actor stopped");
            }
            Ok(())
        });
        let _ = stop_receiver.wait_for(|stop| *stop).await?;
        Ok(())
    })
    .await
}

pub async fn run_fetcher(
    ctx: &ctx::Ctx,
    cfg: Option<(Config, Secrets)>,
    pool: ConnectionPool<Core>,
    sync_state: SyncState,
    main_node_client: Arc<dyn MainNodeClient>,
    action_queue_sender: ActionQueueSender,
    mut stop_receiver: watch::Receiver<bool>,
) -> anyhow::Result<()> {
    let fetcher = Fetcher {
        store: Store(pool),
        sync_state: sync_state.clone(),
        client: main_node_client,
        limiter: limiter::Limiter::new(
            ctx,
            limiter::Rate {
                burst: 10,
                refresh: time::Duration::milliseconds(30),
            },
        ),
    };
    let actions = action_queue_sender;
    scope::run!(&ctx, |ctx, s| async {
        s.spawn_bg(async {
            let res = match cfg {
                Some((cfg, secrets)) => fetcher.run_p2p(ctx, actions, cfg.p2p(&secrets)?).await,
                None => fetcher.run_centralized(ctx, actions).await,
            };
            tracing::info!("Consensus actor stopped");
            res
        });
        ctx.wait(stop_receiver.wait_for(|stop| *stop)).await??;
        Ok(())
    })
    .await
    .context("consensus actor")
}

/// Main node consensus config.
#[derive(Debug, Clone)]
pub struct MainNodeConfig {
    pub executor: executor::Config,
    pub validator_key: validator::SecretKey,
}

impl MainNodeConfig {
    /// Task generating consensus certificates for the miniblocks generated by `StateKeeper`.
    /// Broadcasts the blocks with certificates to gossip network peers.
    pub async fn run(self, ctx: &ctx::Ctx, store: Store) -> anyhow::Result<()> {
        scope::run!(&ctx, |ctx, s| async {
            let mut block_store = store.clone().into_block_store();
            block_store
                .try_init_genesis(ctx, &self.validator_key.public())
                .await
                .wrap("block_store.try_init_genesis()")?;
            let (block_store, runner) = BlockStore::new(ctx, Box::new(block_store))
                .await
                .wrap("BlockStore::new()")?;
            s.spawn_bg(runner.run(ctx));
            let executor = executor::Executor {
                config: self.executor,
                block_store,
                validator: Some(executor::Validator {
                    key: self.validator_key,
                    replica_store: Box::new(store.clone()),
                    payload_manager: Box::new(store.clone()),
                }),
            };
            executor.run(ctx).await
        })
        .await
    }
}
